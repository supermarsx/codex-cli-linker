# Generated by codex-cli-linker (example)
# Date: 2025-09-01
# Usage: run Codex with this profile â†’  codex --profile lmstudio
# Notes: The linker omits empty/blank fields; this example shows typical values.

model = "qwen2.5-coder"
model_provider = "lmstudio"
approval_policy = "on-failure"
sandbox_mode = "workspace-write"
file_opener = "vscode"
model_reasoning_effort = "low"
model_reasoning_summary = "auto"
model_verbosity = "medium"
profile = "lmstudio"
model_context_window = 32000
model_max_output_tokens = 0
project_doc_max_bytes = 1048576
hide_agent_reasoning = false
show_raw_agent_reasoning = true
model_supports_reasoning_summaries = false
preferred_auth_method = "apikey"
disable_response_storage = false

# Optional (commonly omitted when empty):
# chatgpt_base_url = ""
# experimental_resume = ""
# experimental_instructions_file = ""
# experimental_use_exec_command_tool = false
# responses_originator_header_internal_override = ""

[sandbox_workspace_write]
writable_roots = []
network_access = false
exclude_tmpdir_env_var = false
exclude_slash_tmp = false

[tools]
web_search = false

[history]
persistence = "save-all"
max_bytes = 0

[model_providers.lmstudio]
name = "LM Studio"
base_url = "http://localhost:1234/v1"
wire_api = "chat"
request_max_retries = 4
stream_max_retries = 10
stream_idle_timeout_ms = 300000

# Uncomment to add query params (example)
# [model_providers.lmstudio.query_params]
# "api-version" = "2024-06-01"

[profiles.lmstudio]
model = "qwen2.5-coder"
model_provider = "lmstudio"
model_context_window = 32000
model_max_output_tokens = 0
approval_policy = "on-failure"

# --- Alternative: Ollama (comment out the lmstudio blocks above and uncomment below) ---
# model = "llama3.1:8b-instruct"
# model_provider = "ollama"
# profile = "ollama"
#
# [model_providers.ollama]
# name = "Ollama"
# base_url = "http://localhost:11434/v1"
# wire_api = "chat"
# request_max_retries = 4
# stream_max_retries = 10
# stream_idle_timeout_ms = 300000
#
# [profiles.ollama]
# model = "llama3.1:8b-instruct"
# model_provider = "ollama"
# model_context_window = 0
# model_max_output_tokens = 0
# approval_policy = "on-failure"
